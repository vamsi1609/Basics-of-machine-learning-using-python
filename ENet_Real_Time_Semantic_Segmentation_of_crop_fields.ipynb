{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ENet - Real Time Semantic Segmentation of crop fields.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamsi1609/Basics-of-machine-learning-using-python/blob/master/ENet_Real_Time_Semantic_Segmentation_of_crop_fields.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teCWlsWVFjpb",
        "colab_type": "text"
      },
      "source": [
        "# Semantic Segmentation of UAV crop field images using E-net \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rY_EJgl5Uvno",
        "colab_type": "code",
        "outputId": "fdcff8fa-0440-4a91-cada-fb284dfcfef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qnyf_pzhKXv",
        "colab_type": "text"
      },
      "source": [
        "## Install the dependencies and Import them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NCTHdEqj317",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "torch.manual_seed(0)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import cv2\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "from os import listdir"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i26TZVXmhewY",
        "colab_type": "text"
      },
      "source": [
        "## Create the ENet model\n",
        "\n",
        "We decided to to split the model to three sub classes:\n",
        "\n",
        "1) Initial block  \n",
        "\n",
        "2) RDDNeck - class for regular, downsampling and dilated bottlenecks\n",
        "\n",
        "3) ASNeck -  class for asymetric bottlenecks\n",
        "\n",
        "4) UBNeck - class for upsampling bottlenecks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqHUezLfPBwn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class InitialBlock(nn.Module):\n",
        "  \n",
        "  # Initial block of the model:\n",
        "  #         Input\n",
        "  #        /     \\\n",
        "  #       /       \\\n",
        "  #maxpool2d    conv2d-3x3\n",
        "  #       \\       /  \n",
        "  #        \\     /\n",
        "  #      concatenate\n",
        "   \n",
        "    def __init__ (self,in_channels = 3,out_channels = 13):\n",
        "        super().__init__()\n",
        "\n",
        "\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=2, \n",
        "                                      stride = 2, \n",
        "                                      padding = 0)\n",
        "\n",
        "        self.conv = nn.Conv2d(in_channels, \n",
        "                                out_channels,\n",
        "                                kernel_size = 3,\n",
        "                                stride = 2, \n",
        "                                padding = 1)\n",
        "\n",
        "        self.prelu = nn.PReLU(16)\n",
        "\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels)\n",
        "  \n",
        "    def forward(self, x):\n",
        "        \n",
        "        main = self.conv(x)\n",
        "        main = self.batchnorm(main)        \n",
        "        side = self.maxpool(x)\n",
        "        \n",
        "        # concatenating on the channels axis\n",
        "        x = torch.cat((main, side), dim=1)\n",
        "        x = self.prelu(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERqXRpl_sfSE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class UBNeck(nn.Module):\n",
        "    \n",
        "  # Upsampling bottleneck:\n",
        "  #     Bottleneck Input\n",
        "  #        /        \\\n",
        "  #       /          \\\n",
        "  # conv2d-1x1     convTrans2d-1x1\n",
        "  #      |             | PReLU\n",
        "  #      |         convTrans2d-3x3\n",
        "  #      |             | PReLU\n",
        "  #      |         convTrans2d-1x1\n",
        "  #      |             |\n",
        "  # maxunpool2d    Regularizer\n",
        "  #       \\           /  \n",
        "  #        \\         /\n",
        "  #      Summing + PReLU\n",
        "  #\n",
        "  #  Params: \n",
        "  #  projection_ratio - ratio between input and output channels\n",
        "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
        "  \n",
        "    def __init__(self, in_channels, out_channels, relu=False, projection_ratio=4):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        # Define class variables\n",
        "        self.in_channels = in_channels\n",
        "        self.reduced_depth = int(in_channels / projection_ratio)\n",
        "        self.out_channels = out_channels\n",
        "        \n",
        "        \n",
        "        if relu:\n",
        "            activation = nn.ReLU()\n",
        "        else:\n",
        "            activation = nn.PReLU()\n",
        "        \n",
        "        self.unpool = nn.MaxUnpool2d(kernel_size = 2,\n",
        "                                     stride = 2)\n",
        "        \n",
        "        self.main_conv = nn.Conv2d(in_channels = self.in_channels,\n",
        "                                    out_channels = self.out_channels,\n",
        "                                    kernel_size = 1)\n",
        "        \n",
        "        self.dropout = nn.Dropout2d(p=0.1)\n",
        "        \n",
        "        \n",
        "        self.convt1 = nn.ConvTranspose2d(in_channels = self.in_channels,\n",
        "                               out_channels = self.reduced_depth,\n",
        "                               kernel_size = 1,\n",
        "                               padding = 0,\n",
        "                               bias = False)\n",
        "        \n",
        "        \n",
        "        self.prelu1 = activation\n",
        "        \n",
        "        # This layer used for Upsampling\n",
        "        self.convt2 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
        "                                  out_channels = self.reduced_depth,\n",
        "                                  kernel_size = 3,\n",
        "                                  stride = 2,\n",
        "                                  padding = 1,\n",
        "                                  output_padding = 1,\n",
        "                                  bias = False)\n",
        "        \n",
        "        self.prelu2 = activation\n",
        "        \n",
        "        self.convt3 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n",
        "                                  out_channels = self.out_channels,\n",
        "                                  kernel_size = 1,\n",
        "                                  padding = 0,\n",
        "                                  bias = False)\n",
        "        \n",
        "        self.prelu3 = activation\n",
        "        \n",
        "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
        "        \n",
        "    def forward(self, x, indices):\n",
        "        x_copy = x\n",
        "        \n",
        "        # Side Branch\n",
        "        x = self.convt1(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.prelu1(x)\n",
        "        \n",
        "        x = self.convt2(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.prelu2(x)\n",
        "        \n",
        "        x = self.convt3(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        \n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Main Branch\n",
        "        \n",
        "        x_copy = self.main_conv(x_copy)\n",
        "        x_copy = self.unpool(x_copy, indices, output_size=x.size())\n",
        "        \n",
        "        # summing the main and side branches\n",
        "        x = x + x_copy\n",
        "        x = self.prelu3(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-nTIAS9bd9z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RDDNeck(nn.Module):\n",
        "    def __init__(self, dilation, in_channels, out_channels, down_flag, relu=False, projection_ratio=4, p=0.1):\n",
        "      \n",
        "  # Regular|Dilated|Downsampling bottlenecks:\n",
        "  #\n",
        "  #     Bottleneck Input\n",
        "  #        /        \\\n",
        "  #       /          \\\n",
        "  # maxpooling2d   conv2d-1x1\n",
        "  #      |             | PReLU\n",
        "  #      |         conv2d-3x3\n",
        "  #      |             | PReLU\n",
        "  #      |         conv2d-1x1\n",
        "  #      |             |\n",
        "  #  Padding2d     Regularizer\n",
        "  #       \\           /  \n",
        "  #        \\         /\n",
        "  #      Summing + PReLU\n",
        "  #\n",
        "  # Params: \n",
        "  #  dilation (bool) - if True: creating dilation bottleneck\n",
        "  #  down_flag (bool) - if True: creating downsampling bottleneck\n",
        "  #  projection_ratio - ratio between input and output channels\n",
        "  #  relu - if True: relu used as the activation function else: Prelu us used\n",
        "  #  p - dropout ratio\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        # Define class variables\n",
        "        self.in_channels = in_channels\n",
        "        \n",
        "        self.out_channels = out_channels\n",
        "        self.dilation = dilation\n",
        "        self.down_flag = down_flag\n",
        "        \n",
        "        # calculating the number of reduced channels\n",
        "        if down_flag:\n",
        "            self.stride = 2\n",
        "            self.reduced_depth = int(in_channels // projection_ratio)\n",
        "        else:\n",
        "            self.stride = 1\n",
        "            self.reduced_depth = int(out_channels // projection_ratio)\n",
        "        \n",
        "        if relu:\n",
        "            activation = nn.ReLU()\n",
        "        else:\n",
        "            activation = nn.PReLU()\n",
        "        \n",
        "        self.maxpool = nn.MaxPool2d(kernel_size = 2,\n",
        "                                      stride = 2,\n",
        "                                      padding = 0, return_indices=True)\n",
        "        \n",
        "\n",
        "        \n",
        "        self.dropout = nn.Dropout2d(p=p)\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
        "                               out_channels = self.reduced_depth,\n",
        "                               kernel_size = 1,\n",
        "                               stride = 1,\n",
        "                               padding = 0,\n",
        "                               bias = False,\n",
        "                               dilation = 1)\n",
        "        \n",
        "        self.prelu1 = activation\n",
        "        \n",
        "        self.conv2 = nn.Conv2d(in_channels = self.reduced_depth,\n",
        "                                  out_channels = self.reduced_depth,\n",
        "                                  kernel_size = 3,\n",
        "                                  stride = self.stride,\n",
        "                                  padding = self.dilation,\n",
        "                                  bias = True,\n",
        "                                  dilation = self.dilation)\n",
        "                                  \n",
        "        self.prelu2 = activation\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
        "                                  out_channels = self.out_channels,\n",
        "                                  kernel_size = 1,\n",
        "                                  stride = 1,\n",
        "                                  padding = 0,\n",
        "                                  bias = False,\n",
        "                                  dilation = 1)\n",
        "        \n",
        "        self.prelu3 = activation\n",
        "        \n",
        "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        bs = x.size()[0]\n",
        "        x_copy = x\n",
        "        \n",
        "        # Side Branch\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.prelu1(x)\n",
        "        \n",
        "        x = self.conv2(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.prelu2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "        x = self.batchnorm2(x)\n",
        "                \n",
        "        x = self.dropout(x)\n",
        "        \n",
        "        # Main Branch\n",
        "        if self.down_flag:\n",
        "            x_copy, indices = self.maxpool(x_copy)\n",
        "          \n",
        "        if self.in_channels != self.out_channels:\n",
        "            out_shape = self.out_channels - self.in_channels\n",
        "            \n",
        "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
        "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
        "            if torch.cuda.is_available():\n",
        "                extras = extras.cuda()\n",
        "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
        "\n",
        "        # Summing main and side branches\n",
        "        x = x + x_copy\n",
        "        x = self.prelu3(x)\n",
        "        \n",
        "        if self.down_flag:\n",
        "            return x, indices\n",
        "        else:\n",
        "            return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb_i1sCvtmMF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ASNeck(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, projection_ratio=4):\n",
        "      \n",
        "  # Asymetric bottleneck:\n",
        "  #\n",
        "  #     Bottleneck Input\n",
        "  #        /        \\\n",
        "  #       /          \\\n",
        "  #      |         conv2d-1x1\n",
        "  #      |             | PReLU\n",
        "  #      |         conv2d-1x5\n",
        "  #      |             |\n",
        "  #      |         conv2d-5x1\n",
        "  #      |             | PReLU\n",
        "  #      |         conv2d-1x1\n",
        "  #      |             |\n",
        "  #  Padding2d     Regularizer\n",
        "  #       \\           /  \n",
        "  #        \\         /\n",
        "  #      Summing + PReLU\n",
        "  #\n",
        "  # Params:    \n",
        "  #  projection_ratio - ratio between input and output channels\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        # Define class variables\n",
        "        self.in_channels = in_channels\n",
        "        self.reduced_depth = int(in_channels / projection_ratio)\n",
        "        self.out_channels = out_channels\n",
        "        \n",
        "        self.dropout = nn.Dropout2d(p=0.1)\n",
        "        \n",
        "        self.conv1 = nn.Conv2d(in_channels = self.in_channels,\n",
        "                               out_channels = self.reduced_depth,\n",
        "                               kernel_size = 1,\n",
        "                               stride = 1,\n",
        "                               padding = 0,\n",
        "                               bias = False)\n",
        "        \n",
        "        self.prelu1 = nn.PReLU()\n",
        "        \n",
        "        self.conv21 = nn.Conv2d(in_channels = self.reduced_depth,\n",
        "                                  out_channels = self.reduced_depth,\n",
        "                                  kernel_size = (1, 5),\n",
        "                                  stride = 1,\n",
        "                                  padding = (0, 2),\n",
        "                                  bias = False)\n",
        "        \n",
        "        self.conv22 = nn.Conv2d(in_channels = self.reduced_depth,\n",
        "                                  out_channels = self.reduced_depth,\n",
        "                                  kernel_size = (5, 1),\n",
        "                                  stride = 1,\n",
        "                                  padding = (2, 0),\n",
        "                                  bias = False)\n",
        "        \n",
        "        self.prelu2 = nn.PReLU()\n",
        "        \n",
        "        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,\n",
        "                                  out_channels = self.out_channels,\n",
        "                                  kernel_size = 1,\n",
        "                                  stride = 1,\n",
        "                                  padding = 0,\n",
        "                                  bias = False)\n",
        "        \n",
        "        self.prelu3 = nn.PReLU()\n",
        "        \n",
        "        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(self.out_channels)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        bs = x.size()[0]\n",
        "        x_copy = x\n",
        "        \n",
        "        # Side Branch\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.prelu1(x)\n",
        "        \n",
        "        x = self.conv21(x)\n",
        "        x = self.conv22(x)\n",
        "        x = self.batchnorm(x)\n",
        "        x = self.prelu2(x)\n",
        "        \n",
        "        x = self.conv3(x)\n",
        "                \n",
        "        x = self.dropout(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        \n",
        "        # Main Branch\n",
        "        \n",
        "        if self.in_channels != self.out_channels:\n",
        "            out_shape = self.out_channels - self.in_channels\n",
        "            \n",
        "            #padding and concatenating in order to match the channels axis of the side and main branches\n",
        "            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n",
        "            if torch.cuda.is_available():\n",
        "                extras = extras.cuda()\n",
        "            x_copy = torch.cat((x_copy, extras), dim = 1)\n",
        "        \n",
        "        # Summing main and side branches\n",
        "        x = x + x_copy\n",
        "        x = self.prelu3(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1pz_bve690y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ENet(nn.Module):\n",
        "  \n",
        "  # Creating Enet model!\n",
        "  \n",
        "    def __init__(self, C):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Define class variables\n",
        "        # C - number of classes\n",
        "        self.C = C\n",
        "        \n",
        "        # The initial block\n",
        "        self.init = InitialBlock()\n",
        "        \n",
        "        \n",
        "        # The first bottleneck\n",
        "        self.b10 = RDDNeck(dilation=1, \n",
        "                           in_channels=16, \n",
        "                           out_channels=64, \n",
        "                           down_flag=True, \n",
        "                           p=0.01)\n",
        "        \n",
        "        self.b11 = RDDNeck(dilation=1, \n",
        "                           in_channels=64, \n",
        "                           out_channels=64, \n",
        "                           down_flag=False, \n",
        "                           p=0.01)\n",
        "        \n",
        "        self.b12 = RDDNeck(dilation=1, \n",
        "                           in_channels=64, \n",
        "                           out_channels=64, \n",
        "                           down_flag=False, \n",
        "                           p=0.01)\n",
        "        \n",
        "        self.b13 = RDDNeck(dilation=1, \n",
        "                           in_channels=64, \n",
        "                           out_channels=64, \n",
        "                           down_flag=False, \n",
        "                           p=0.01)\n",
        "        \n",
        "        self.b14 = RDDNeck(dilation=1, \n",
        "                           in_channels=64, \n",
        "                           out_channels=64, \n",
        "                           down_flag=False, \n",
        "                           p=0.01)\n",
        "        \n",
        "        \n",
        "        # The second bottleneck\n",
        "        self.b20 = RDDNeck(dilation=1, \n",
        "                           in_channels=64, \n",
        "                           out_channels=128, \n",
        "                           down_flag=True)\n",
        "        \n",
        "        self.b21 = RDDNeck(dilation=1, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        self.b22 = RDDNeck(dilation=2, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        self.b23 = ASNeck(in_channels=128, \n",
        "                          out_channels=128)\n",
        "        \n",
        "        self.b24 = RDDNeck(dilation=4, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        self.b25 = RDDNeck(dilation=1, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        self.b26 = RDDNeck(dilation=8, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        self.b27 = ASNeck(in_channels=128, \n",
        "                          out_channels=128)\n",
        "        \n",
        "        self.b28 = RDDNeck(dilation=16, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        \n",
        "        # The third bottleneck\n",
        "        self.b31 = RDDNeck(dilation=1, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        self.b32 = RDDNeck(dilation=2, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        self.b33 = ASNeck(in_channels=128, \n",
        "                          out_channels=128)\n",
        "        \n",
        "        self.b34 = RDDNeck(dilation=4, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        self.b35 = RDDNeck(dilation=1, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        self.b36 = RDDNeck(dilation=8, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        self.b37 = ASNeck(in_channels=128, \n",
        "                          out_channels=128)\n",
        "        \n",
        "        self.b38 = RDDNeck(dilation=16, \n",
        "                           in_channels=128, \n",
        "                           out_channels=128, \n",
        "                           down_flag=False)\n",
        "        \n",
        "        \n",
        "        # The fourth bottleneck\n",
        "        self.b40 = UBNeck(in_channels=128, \n",
        "                          out_channels=64, \n",
        "                          relu=True)\n",
        "        \n",
        "        self.b41 = RDDNeck(dilation=1, \n",
        "                           in_channels=64, \n",
        "                           out_channels=64, \n",
        "                           down_flag=False, \n",
        "                           relu=True)\n",
        "        \n",
        "        self.b42 = RDDNeck(dilation=1, \n",
        "                           in_channels=64, \n",
        "                           out_channels=64, \n",
        "                           down_flag=False, \n",
        "                           relu=True)\n",
        "        \n",
        "        \n",
        "        # The fifth bottleneck\n",
        "        self.b50 = UBNeck(in_channels=64, \n",
        "                          out_channels=16, \n",
        "                          relu=True)\n",
        "        \n",
        "        self.b51 = RDDNeck(dilation=1, \n",
        "                           in_channels=16, \n",
        "                           out_channels=16, \n",
        "                           down_flag=False, \n",
        "                           relu=True)\n",
        "        \n",
        "        \n",
        "        # Final ConvTranspose Layer\n",
        "        self.fullconv = nn.ConvTranspose2d(in_channels=16, \n",
        "                                           out_channels=self.C, \n",
        "                                           kernel_size=3, \n",
        "                                           stride=2, \n",
        "                                           padding=1, \n",
        "                                           output_padding=1,\n",
        "                                           bias=False)\n",
        "        \n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        # The initial block\n",
        "        x = self.init(x)\n",
        "        \n",
        "        # The first bottleneck\n",
        "        x, i1 = self.b10(x)\n",
        "        x = self.b11(x)\n",
        "        x = self.b12(x)\n",
        "        x = self.b13(x)\n",
        "        x = self.b14(x)\n",
        "        \n",
        "        # The second bottleneck\n",
        "        x, i2 = self.b20(x)\n",
        "        x = self.b21(x)\n",
        "        x = self.b22(x)\n",
        "        x = self.b23(x)\n",
        "        x = self.b24(x)\n",
        "        x = self.b25(x)\n",
        "        x = self.b26(x)\n",
        "        x = self.b27(x)\n",
        "        x = self.b28(x)\n",
        "        \n",
        "        # The third bottleneck\n",
        "        x = self.b31(x)\n",
        "        x = self.b32(x)\n",
        "        x = self.b33(x)\n",
        "        x = self.b34(x)\n",
        "        x = self.b35(x)\n",
        "        x = self.b36(x)\n",
        "        x = self.b37(x)\n",
        "        x = self.b38(x)\n",
        "        \n",
        "        # The fourth bottleneck\n",
        "        x = self.b40(x, i2)\n",
        "        x = self.b41(x)\n",
        "        x = self.b42(x)\n",
        "        \n",
        "        # The fifth bottleneck\n",
        "        x = self.b50(x, i1)\n",
        "        x = self.b51(x)\n",
        "        \n",
        "        # Final ConvTranspose Layer\n",
        "        x = self.fullconv(x)\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jg1Rnb3uhnxR",
        "colab_type": "text"
      },
      "source": [
        "## Instantiate the ENet model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UdSwlKiB27K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "enet = ENet(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5kaK6CnhwG_",
        "colab_type": "text"
      },
      "source": [
        "Move the model to cuda if available"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lZcgE-F_hvxX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking if there is any gpu available and pass the model to gpu or cpu\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "enet = enet.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvURoRuSlyvP",
        "colab_type": "text"
      },
      "source": [
        "## Define the loader that will load the input and output images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFCIsDF2bpQx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loader(training_path, segmented_path, batch_size, h=320, w=1000):\n",
        "    filenames_t = os.listdir(training_path)\n",
        "    total_files_t = len(filenames_t)\n",
        "    \n",
        "    filenames_s = os.listdir(segmented_path)\n",
        "    total_files_s = len(filenames_s)\n",
        "    \n",
        "    assert(total_files_t == total_files_s)\n",
        "    \n",
        "    if str(batch_size).lower() == 'all':\n",
        "        batch_size = total_files_s\n",
        "    \n",
        "    idx = 0\n",
        "    while(1):\n",
        "      # Choosing random indexes of images and labels\n",
        "        batch_idxs = np.random.randint(0, total_files_s, batch_size)\n",
        "            \n",
        "        \n",
        "        inputs = []\n",
        "        labels = []\n",
        "        \n",
        "        for jj in batch_idxs:\n",
        "          # Reading normalized photo\n",
        "            img = plt.imread(training_path + filenames_t[jj])\n",
        "          # Resizing using nearest neighbor method\n",
        "            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n",
        "            inputs.append(img)\n",
        "            \n",
        "          # Reading semantic image\n",
        "            img = Image.open(segmented_path + filenames_s[jj])\n",
        "            img = np.array(img)\n",
        "            \n",
        "          # Resizing using nearest neighbor method\n",
        "            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n",
        "            labels.append(img)\n",
        "            \n",
        "         \n",
        "        inputs = np.stack(inputs, axis=2)\n",
        "      # Changing image format to C x H x W\n",
        "        inputs = torch.tensor(inputs).transpose(0, 2).transpose(1, 3)\n",
        "        \n",
        "        labels = torch.tensor(labels)\n",
        "        \n",
        "        yield inputs, labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4qD8VBah2K2",
        "colab_type": "text"
      },
      "source": [
        "## Define the class weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1RPIg4LyGjr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmnNEeMHiSU2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_class_weights(num_classes, c=0.5):\n",
        "    pipe = loader('/content/drive/My Drive/train/', '/content/drive/My Drive/trainannot/', batch_size='all')\n",
        "    _, labels = next(pipe)\n",
        "    all_labels = labels.flatten()\n",
        "    each_class = np.bincount(all_labels, minlength=num_classes)\n",
        "    prospensity_score = each_class / len(all_labels)\n",
        "    class_weights = 1 / (np.log(c + prospensity_score))\n",
        "    return class_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHBoLmadmrA_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_weights = get_class_weights(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wwnNuFIfhsXm",
        "colab_type": "text"
      },
      "source": [
        "## Define the Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI9425iz7thP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 5e-5\n",
        "batch_size = 15\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n",
        "optimizer = torch.optim.Adam(enet.parameters(), \n",
        "                             lr=lr,\n",
        "                             weight_decay=1.2e-5)\n",
        "\n",
        "print_every = 5\n",
        "eval_every = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFIdlVWviBYl",
        "colab_type": "text"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ6XJzl6Ta1_",
        "colab_type": "code",
        "outputId": "e332a88c-0960-4b74-cd41-3e33ca1d198a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "train_losses = []\n",
        "eval_losses = []\n",
        "\n",
        "bc_train = 20 // batch_size # mini_batch train\n",
        "bc_eval = 10 // batch_size  # mini_batch validation\n",
        "\n",
        "# Define pipeline objects\n",
        "pipe = loader('/content/drive/My Drive/train/', '/content/drive/My Drive/trainannot/', batch_size)\n",
        "eval_pipe = loader('/content/drive/My Drive/val/', '/content/drive/My Drive/valannot/', batch_size)\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "# Train loop\n",
        "i=0\n",
        "for e in range(1, epochs+1):\n",
        "    \n",
        "    \n",
        "    train_loss = 0\n",
        "    print ('-'*15,'Epoch %d' % e, '-'*15)\n",
        "    \n",
        "    enet.train()\n",
        "    \n",
        "    for _ in tqdm(range(bc_train)):\n",
        "        i+=1\n",
        "        X_batch, mask_batch = next(pipe)\n",
        "        #print(mask_batch.size())\n",
        "        # assign data to cpu/gpu\n",
        "        X_batch, mask_batch = X_batch.to(device), mask_batch.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out = enet(X_batch.float())\n",
        "\n",
        "        \n",
        "        # loss calculation\n",
        "        loss = criterion(out, mask_batch.long())\n",
        "        # update weights\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        \n",
        "    print ()\n",
        "    train_losses.append(train_loss)\n",
        "    \n",
        "    if (e+1) % print_every == 0:\n",
        "        print ('Epoch {}/{}...'.format(e, epochs),\n",
        "                'Loss {:6f}'.format(train_loss))\n",
        "    \n",
        "    if e % eval_every == 0:\n",
        "        with torch.no_grad():\n",
        "            enet.eval()\n",
        "            \n",
        "            eval_loss = 0\n",
        "\n",
        "            # Validation loop\n",
        "            for _ in tqdm(range(bc_eval)):\n",
        "                inputs, labels = next(eval_pipe)\n",
        "\n",
        "                \n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    \n",
        "                \n",
        "                out = enet(inputs)\n",
        "                \n",
        "                out = out.data.max(1)[1]\n",
        "                \n",
        "                eval_loss += (labels.long() - out.long()).sum()\n",
        "                \n",
        "            \n",
        "            print ()\n",
        "            print ('Loss {:6f}'.format(eval_loss))\n",
        "            \n",
        "            eval_losses.append(eval_loss)\n",
        "        \n",
        "    if e % print_every == 0:\n",
        "        checkpoint = {\n",
        "            'epochs' : e,\n",
        "            'state_dict' : enet.state_dict()\n",
        "        }\n",
        "       # torch.save(checkpoint, '/content/ckpt-enet-{}-{}.pth'.format(e, train_loss))\n",
        "        #print ('Model saved!')\n",
        "\n",
        "print ('Epoch {}/{}...'.format(e, epochs),\n",
        "       'Total Mean Loss: {:6f}'.format(sum(train_losses) / epochs))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "--------------- Epoch 1 ---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-741b99bcf541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-50-c3a92e5cbf9f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m         \u001b[0;31m# The initial block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;31m# The first bottleneck\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-716d851275ec>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mmain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatchnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mside\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaxpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    344\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    345\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 346\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 56.00 MiB (GPU 0; 7.43 GiB total capacity; 6.66 GiB already allocated; 12.94 MiB free; 6.90 GiB reserved in total by PyTorch)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dWM6D5Rl_Mq",
        "colab_type": "text"
      },
      "source": [
        "## Use the code to infer on new images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8pxajTHj5vNG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fname = 'a (1).png'\n",
        "tmg_ = plt.imread('/content/drive/My Drive/testrgb/' + fname)\n",
        "tmg_ = cv2.resize(tmg_, (512, 512), cv2.INTER_NEAREST)\n",
        "tmg = torch.tensor(tmg_).unsqueeze(0).float()\n",
        "tmg = tmg.transpose(2, 3).transpose(1, 2).to(device)\n",
        "\n",
        "enet.to(device)\n",
        "with torch.no_grad():\n",
        "    out1 = enet(tmg.float()).squeeze(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8vh2eozpAAh",
        "colab_type": "text"
      },
      "source": [
        "## Load the label image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8zQiSaeeo_rZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smg_ = Image.open('/content/drive/My Drive/testannot/' + 'a (1).png')\n",
        "smg_ = cv2.resize(np.array(smg_), (512, 512), cv2.INTER_NEAREST)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EXLLTnYpF2D",
        "colab_type": "text"
      },
      "source": [
        "## Move the output to cpu and convert it to numpy and see how each class looks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjpnkPB6RKub",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out2 = out1.cpu().detach().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-sVZCMU6vgF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "mno = 2 # Should be between 0 - n-1 | where n is the number of classes\n",
        "\n",
        "figure = plt.figure(figsize=(20, 10))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Input Image')\n",
        "plt.axis('off')\n",
        "plt.imshow(tmg_)\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Output Image')\n",
        "plt.axis('off')\n",
        "plt.imshow(out2[mno,:,:])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8yIfMvrpZkT",
        "colab_type": "text"
      },
      "source": [
        "Get the class labels from the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1C9bmgYxkga",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b_ = out1.data.max(0)[1].cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLw_Y8emp9HM",
        "colab_type": "text"
      },
      "source": [
        "Define the function that maps a 2D image with all the class labels to a segmented image with the specified colored maps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkyoipIGZKk9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def decode_segmap(image):\n",
        "    Sky = [0, 0, 0]\n",
        "    Building = [0, 0, 255]\n",
        "    Pole = [0, 255, 0]\n",
        "\n",
        "    label_colours = np.array([Sky, Building, Pole\n",
        "                             ]).astype(np.uint8)\n",
        "    r = np.zeros_like(image).astype(np.uint8)\n",
        "    g = np.zeros_like(image).astype(np.uint8)\n",
        "    b = np.zeros_like(image).astype(np.uint8)\n",
        "    for l in range(0, 3):\n",
        "        r[image == l] = label_colours[l, 0]\n",
        "        g[image == l] = label_colours[l, 1]\n",
        "        b[image == l] = label_colours[l, 2]\n",
        "\n",
        "    rgb = np.zeros((image.shape[0], image.shape[1], 3)).astype(np.uint8)\n",
        "    rgb[:, :, 0] = b\n",
        "    rgb[:, :, 1] = g\n",
        "    rgb[:, :, 2] = r\n",
        "    return rgb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_B6n9VmqP5g",
        "colab_type": "text"
      },
      "source": [
        "Decode the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHyCu5TTaNkv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_seg = decode_segmap(smg_)\n",
        "pred_seg = decode_segmap(b_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G_8C40YXziZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "figure = plt.figure(figsize=(20, 10))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Input Image')\n",
        "plt.axis('off')\n",
        "plt.imshow(tmg_)\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Predicted Segmentation')\n",
        "plt.axis('off')\n",
        "plt.imshow(pred_seg)\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title('Ground Truth')\n",
        "plt.axis('off')\n",
        "plt.imshow(true_seg)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YI65NDYmr7h",
        "colab_type": "text"
      },
      "source": [
        "## Save the model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjUZDGfU5F8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the parameters\n",
        "checkpoint = {\n",
        "    'epochs' : e,\n",
        "    'state_dict' : enet.state_dict()\n",
        "}\n",
        "torch.save(checkpoint, 'ckpt-enet-1.pth')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51UgRcNYmwCc",
        "colab_type": "text"
      },
      "source": [
        "## Save the entire model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQVVoXhn5oun",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Save the model\n",
        "torch.save(enet, '/content/model.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK8VooDHz9zN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iou_sum=0\n",
        "path = \"/content/drive/My Drive/testrgb/\"\n",
        "imagesList = listdir(path)\n",
        "for image in imagesList:\n",
        "        tmg_ = plt.imread(path+image)\n",
        "        tmg_ = cv2.resize(tmg_, (512, 512), cv2.INTER_NEAREST)\n",
        "        tmg_ = torch.tensor(tmg_).unsqueeze(0).float()\n",
        "        tmg_ = tmg.transpose(2, 3).transpose(1, 2).to(device)\n",
        "        enet.to(device)\n",
        "        with torch.no_grad():\n",
        "          out1 = enet(tmg.float()).squeeze(0)\n",
        "        smg_ = Image.open('/content/drive/My Drive/testannot/'+image)\n",
        "        smg_ = cv2.resize(np.array(smg_), (512, 512), cv2.INTER_NEAREST)\n",
        "        b_ = out1.data.max(0)[1].cpu().numpy()\n",
        "        true_seg = decode_segmap(smg_)\n",
        "        pred_seg = decode_segmap(b_)\n",
        "        intersection = np.logical_and(true_seg, pred_seg)\n",
        "        union = np.logical_or(true_seg, pred_seg)\n",
        "        iou_score = np.sum(intersection) / np.sum(union)\n",
        "        iou_sum+=iou_score\n",
        "print(iou_sum/66)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCtgo4vIkmVd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iou_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgxFaMDblffX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}